{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rToK0Tku8PPn"
   },
   "source": [
    "## makemore: becoming a backprop ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8sFElPqq8PPp"
   },
   "outputs": [],
   "source": [
    "# there no change change in the first several cells from last lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ChBbac4y8PPq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3362, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3607, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/z5/81gv5lc94lg2dbfrv548wwr80000gn/T/ipykernel_22929/3578514319.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/dev/ml/makemore/makemore_p4_manual_backprop/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x6GhEWW18aCS"
   },
   "outputs": [],
   "source": [
    "# download the names.txt file from github\n",
    "#!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
    "\n",
    "# (I already have it in my local project directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "klmu3ZG08PPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BCQomLE_8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V_zt2QHr8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []\n",
    "\n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eg20-vsg8PPt"
   },
   "outputs": [],
   "source": [
    "# ok biolerplate done, now we get to the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MJPU8HT08PPu"
   },
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZlFLjQyT8PPu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QY-y96Y48PPv"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8ofj1s6d8PPv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3440, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum.shape, counts.shape\n",
    "# [a11,a12,a13\n",
    "#  a21,a22,a23\n",
    "#  a31,a32,a33\n",
    "#].sum\n",
    "# = [a11+a12+a23\n",
    "#    a21+a22+a23\n",
    "#    ...\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mO-8aqxK8PPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually,\n",
    "# backpropagating through exactly all of the variables\n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# -----------------\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n \n",
    "\n",
    "dprobs = dlogprobs * (1.0/probs)\n",
    "# sum over rows, counts_sum_inv is replicated and multiplied across all cols of counts\n",
    "# (counts_sum_inv.shape = (32,1)), (counts.shape = (32,27)): so you sum gradients of entries in counts_sum_inv,\n",
    "# since each entry in counts_sum_inv is being multiplied 27 times across all cols of counts\n",
    "dcounts_sum_inv = (dprobs * counts).sum(1, keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = dcounts_sum_inv * (-1*counts_sum**-2)\n",
    "dcounts += dcounts_sum\n",
    "dnorm_logits = dcounts * norm_logits.exp()\n",
    "\n",
    "dlogits = dnorm_logits.clone()\n",
    "\n",
    "dlogit_maxes = -(dnorm_logits.sum(1, keepdim=True))\n",
    "\n",
    "dlogits += dlogit_maxes * F.one_hot(logits.max(1).indices, vocab_size)\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0,keepdim=True)\n",
    "dhpreact = (1 - h**2) * dh\n",
    "dbngain = (bnraw*dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = dhpreact * bngain\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbnvar_inv = (dbnraw*bndiff).sum(0,keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar = -.5*(bnvar+1e-5)**-1.5 * dbnvar_inv\n",
    "dbndiff2 = 1/(n-1)*torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += 2 * bndiff * dbndiff2\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "dhprebn = dbndiff.clone()\n",
    "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = torch.ones_like(b1) * dhprebn.sum(0,keepdim=True)\n",
    "demb = dembcat.view(emb.shape)\n",
    "\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[i,j]\n",
    "        dC[ix] += demb[i,j]\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ebLtYji_8PPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3392951488494873 diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it outO\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-gCXbB4C8PPx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.05359673500061e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dlogits = F.softmax(logits,1) # TODO. my solution is 3 lines\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hd-MkhB68PPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "POdeZSKT8PPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0)) # TODO. my solution is 1 (long) line\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wPy8DhqB8PPz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: 12297\n",
      "      0/ 200000: 3.7995\n",
      "  10000/ 200000: 2.1684\n",
      "  20000/ 200000: 2.3897\n",
      "  30000/ 200000: 2.4550\n",
      "  40000/ 200000: 2.0067\n",
      "  50000/ 200000: 2.3294\n",
      "  60000/ 200000: 2.3428\n",
      "  70000/ 200000: 2.0871\n",
      "  80000/ 200000: 2.3833\n",
      "  90000/ 200000: 2.2368\n",
      " 100000/ 200000: 1.9731\n",
      " 110000/ 200000: 2.2695\n",
      " 120000/ 200000: 2.0282\n",
      " 130000/ 200000: 2.5148\n",
      " 140000/ 200000: 2.3303\n",
      " 150000/ 200000: 2.1848\n",
      " 160000/ 200000: 1.9142\n",
      " 170000/ 200000: 1.8515\n",
      " 180000/ 200000: 2.0198\n",
      " 190000/ 200000: 1.8985\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(f\"total params: {sum(p.nelement() for p in parameters)}\") # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "    #print(\"Entering for loop\")\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    #print(\"finished forward pass\")\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "    #print(\"did loss.backward()\")\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    # YOUR CODE HERE :)\n",
    "    #dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
    "    # -----------------\n",
    "    #print(\"starting manual gradient calcs\")\n",
    "    \n",
    "    dlogits = F.softmax(logits,1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # 2nd layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    # tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # batchnorm backprop\n",
    "    dbngain = (bnraw*dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    #1st layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    #embedding\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "        for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k,j]\n",
    "            dC[ix] += demb[k,j]\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    \n",
    "    #print(\"finished manual gradient calcs\")\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZEpI0hMW8PPz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 10)        | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
      "(30, 200)       | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n",
      "(200,)          | exact: False | approximate: True  | maxdiff: 5.9371814131736755e-09\n",
      "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n",
      "(27,)           | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n"
     ]
    }
   ],
   "source": [
    "# useful for checking your gradients\n",
    "for p,g in zip(parameters, grads):\n",
    "  cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KImLWNoh8PP0"
   },
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6aFnP_Zc8PP0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0701935291290283\n",
      "val 2.1109912395477295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x125f2b050>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esWqmhyj8PP1"
   },
   "outputs": [],
   "source": [
    "# I achieved:\n",
    "# train 2.0718822479248047\n",
    "# val 2.1162495613098145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xHeQNv3s8PP1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mora.\n",
      "mayah.\n",
      "see.\n",
      "mad.\n",
      "ryla.\n",
      "rethruthadrie.\n",
      "cailee.\n",
      "melin.\n",
      "shi.\n",
      "jen.\n",
      "eden.\n",
      "estanar.\n",
      "kayzion.\n",
      "kamin.\n",
      "shubergihira.\n",
      "sten.\n",
      "joselle.\n",
      "joseus.\n",
      "kuma.\n",
      "geder.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
